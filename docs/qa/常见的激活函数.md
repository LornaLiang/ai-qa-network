# 常见激活函数（Activation Functions）与作用

> 主线：激活函数的核心作用 = **引入非线性 + 控制输出形式 + 影响梯度/训练稳定性**。

---

## 1. 为什么需要激活函数？

**Q：如果网络里每一层都没有激活（全是线性变换），会发生什么？**

设两层线性：

$$
h = W_1 x + b_1
$$

$$
\hat y = W_2 h + b_2
$$

合并：

$$
\hat y = (W_2 W_1)x + (W_2 b_1 + b_2)
$$

**结论：多层线性仍等价于一层线性**，表达能力不提升。  
因此需要激活函数把线性输出变成非线性：
$$
a = \sigma(z)
$$

---

## 2. 常见激活函数一览（公式 + 输出范围 + 常用场景）

### 2.1 Sigmoid（Logistic）
**定义**

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

**输出范围**
$$
\sigma(z)\in(0,1)
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\sigmoid.png" style="zoom:50%;" />

**作用**

- 把任意实数压缩到 (0,1)，便于解释为“概率”
- 常用于二分类输出层（配合交叉熵）

**主要问题（训练层面）**
- 大幅度正/负输入时趋近 0/1，梯度变小，容易出现梯度消失（深层训练困难）

---

### 2.2 Tanh（双曲正切）
**定义**

$$
\tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}
$$

**输出范围**

$$
\tanh(z)\in(-1,1)
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\tanh.png" style="zoom:50%;" />

**作用**

- 输出以 0 为中心，某些优化场景比 sigmoid 更“对称”

**主要问题**
- 仍可能在饱和区梯度变小，深层仍可能梯度消失

---

### 2.3 ReLU（Rectified Linear Unit）
**定义**

$$
\text{ReLU}(z)=\max(0,z)
$$

**输出范围**

$$
\text{ReLU}(z)\in[0,\infty)
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\relu.png" style="zoom:50%;" />

**作用**

- 计算简单
- 在正区间梯度不衰减（相对 sigmoid/tanh 更利于深层训练）
- 现代深度网络隐藏层最常用

**主要问题**
- 负区间输出恒为 0，可能出现“死亡 ReLU”（某些神经元长期输出 0，难以恢复）

---

### 2.4 Leaky ReLU
**定义**

$$
\text{LeakyReLU}(z)=
\begin{cases}
z, & z\ge 0 \\
\alpha z, & z<0
\end{cases}
$$

其中常见取值：

$$
\alpha \in (0,1)
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\leaky_relu_a0p1.png" alt="leaky_relu_a0p1" style="zoom:50%;" />

**作用**

- 给负区间保留一个小斜率，缓解“死亡 ReLU”

---

### 2.5 PReLU（Parametric ReLU）
**定义**

$$
\text{PReLU}(z)=
\begin{cases}
z, & z\ge 0 \\
a z, & z<0
\end{cases}
$$

**关键点**
- 这里的 $a$ 是可学习参数（由数据学出来）

**作用**

- 比固定的 Leaky ReLU 更灵活，负半轴斜率可自适应

---

### 2.6 ELU（Exponential Linear Unit）
**定义**

$$
\text{ELU}(z)=
\begin{cases}
z, & z\ge 0 \\
\alpha(e^{z}-1), & z<0
\end{cases}
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\elu_a1.png" style="zoom:50%;" />

**作用**

- 负区间输出为负值并平滑，可能让训练更稳定（在某些任务/结构上）
- 比 ReLU 更“平滑”，但计算更贵

---

### 2.7 GELU（Gaussian Error Linear Unit）
**常用近似形式**

$$
\text{GELU}(z)\approx 0.5z\left(1+\tanh\left(\sqrt{\frac{2}{\pi}}\left(z+0.044715z^3\right)\right)\right)
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\gelu_approx.png" style="zoom:50%;" />

**作用**

- 平滑的“门控”式非线性
- 在 Transformer 等结构中非常常见（尤其在 MLP/FFN 部分）

---

### 2.8 Softplus（ReLU 的平滑版本）
**定义**

$$
\text{Softplus}(z)=\ln(1+e^{z})
$$

<img src="E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\softplus.png" style="zoom:50%;" />

**作用**

- 平滑、处处可导
- 在需要平滑性的模型中可能更合适，但比 ReLU 慢

---

### 2.9 Softmax（多分类输出层）
**定义（对向量 $z\in \mathbb{R}^K$）**

$$
\text{softmax}(z_i)=\frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
$$

**输出性质**

$$
\sum_{i=1}^K \text{softmax}(z_i)=1
$$

![](E:\codeManagement\ai-qa-network\docs\image\ActivationFunctions\softmax.png)

**作用**

- 把 $K$ 维打分变成概率分布
- 多分类输出层标准选择（配合交叉熵）

---

